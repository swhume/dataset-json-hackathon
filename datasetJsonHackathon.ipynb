{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset-JSON Hackathon Notebook\n",
    "\n",
    "This notebook was developed as part of the COSA Dataset-JSON Hackathon. It is experimental and meant as an exercise to explore working with Dataset-JSON in Python.\n",
    "\n",
    "This notebook demonstrates using Python Pandas to convert Dataset-JSON files to dataframes that are then stored as a CSV file. Some examples that use streaming to iteratively process the Dataset-JSON files are included to show how files too big to fit into memory could be processed, though this has not been tested with a very large dataset. A Pandas dataframe is then converted back into Dataset-JSON to demonstrate creating a Dataset-JSON file from a dataframe.\n",
    "\n",
    "### Import the Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:25:22.464040650Z",
     "start_time": "2023-08-18T17:25:22.420381234Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ijson\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Dataset-JSON example file from GitHub\n",
    "\n",
    "Retrieve an example Dataset-JSON file from the CDISC DataExchange-Dataset-Json repository and write the file in a local data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:34:21.928885087Z",
     "start_time": "2023-08-18T17:34:21.262951309Z"
    }
   },
   "outputs": [],
   "source": [
    "data_file = 'data/vs.json'\n",
    "try:\n",
    "    r = requests.get('https://github.com/cdisc-org/DataExchange-DatasetJson/blob/master/examples/sdtm/vs.json?raw=True')\n",
    "    r.raise_for_status()\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    raise SystemExit(err)\n",
    "with open(data_file, 'w') as f:\n",
    "    json.dump(r.json(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Dataset-JSON using json module\n",
    "For smaller datasets, simply load data using json module. This loads the entire file into memory so may not work for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:34:42.370659622Z",
     "start_time": "2023-08-18T17:34:42.281642848Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_file, 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Show the name and label for the dataset as well as all the variables names that will be used as column headings. Also, load the data types from the Dataset-JSON file to set the data types in the Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:34:56.071243277Z",
     "start_time": "2023-08-18T17:34:56.061870991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: VS (Vital Signs)\n",
      "\n",
      "Variables: ITEMGROUPDATASEQ, STUDYID, DOMAIN, USUBJID, VSSEQ, VSTESTCD, VSTEST, VSPOS, VSORRES, VSORRESU, VSSTRESC, VSSTRESN, VSSTRESU, VSSTAT, VSLOC, VSLOBXFL, VSREPNUM, VISITNUM, VISIT, EPOCH, VSDTC, VSDY\n"
     ]
    }
   ],
   "source": [
    "dataset_attrs = list(data[\"clinicalData\"][\"itemGroupData\"].values())[0]\n",
    "print(f\"Name: {dataset_attrs['name']} ({dataset_attrs['label']})\", end='\\n\\n')\n",
    "variables = [var['name'] for var in dataset_attrs['items']]\n",
    "print(f\"Variables: {', '.join([var_name for var_name in variables])}\")\n",
    "data_types = [var['type'] for var in dataset_attrs['items']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a dataframe from the Dataset-JSON file\n",
    "Create a dataframe from the Dataset-JSON file. Then print the top 5 rows and provide the memory usage for the dataframe. Then save the dataframe as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:35:09.488089393Z",
     "start_time": "2023-08-18T17:35:09.408510808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ITEMGROUPDATASEQ       STUDYID DOMAIN   USUBJID  VSSEQ VSTESTCD  \\\n",
      "0                 1  CDISCPILOT01     VS  CDISC001      1    DIABP   \n",
      "1                 2  CDISCPILOT01     VS  CDISC001      2    DIABP   \n",
      "2                 3  CDISCPILOT01     VS  CDISC001      3    DIABP   \n",
      "3                 4  CDISCPILOT01     VS  CDISC001      4    DIABP   \n",
      "4                 5  CDISCPILOT01     VS  CDISC001      5    DIABP   \n",
      "\n",
      "                     VSTEST     VSPOS VSORRES VSORRESU  ... VSSTRESU  VSSTAT  \\\n",
      "0  Diastolic Blood Pressure  STANDING      71     mmHg  ...     mmHg           \n",
      "1  Diastolic Blood Pressure  STANDING      71     mmHg  ...     mmHg           \n",
      "2  Diastolic Blood Pressure  STANDING      83     mmHg  ...     mmHg           \n",
      "3  Diastolic Blood Pressure  STANDING      79     mmHg  ...     mmHg           \n",
      "4  Diastolic Blood Pressure  STANDING      68     mmHg  ...     mmHg           \n",
      "\n",
      "  VSLOC VSLOBXFL VSREPNUM VISITNUM        VISIT      EPOCH       VSDTC VSDY  \n",
      "0                     NaN        1  SCREENING 1  SCREENING  2012-11-23   -7  \n",
      "1                     NaN        2  SCREENING 2  SCREENING  2012-11-28   -2  \n",
      "2              Y      NaN        3     BASELINE  SCREENING  2012-11-30    1  \n",
      "3                     NaN        4       WEEK 2  TREATMENT  2012-12-13   14  \n",
      "4                     NaN        5       WEEK 4  TREATMENT  2012-12-26   27  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "\n",
      "dataframe memory usage: 248992 bytes\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset_attrs['itemData'], columns=variables)\n",
    "print(df.head(5), end='\\n\\n')\n",
    "print(f\"\\ndataframe memory usage: {df.memory_usage().sum()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write the dataframe out as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:35:26.979411870Z",
     "start_time": "2023-08-18T17:35:26.936417776Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/' + dataset_attrs['name'] + '.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create datatype conversion functions\n",
    "Create helper functions that convert Dataset-JSON (and Define-XML) datatypes to pandas dataframe datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:35:45.365080829Z",
     "start_time": "2023-08-18T17:35:45.357311389Z"
    }
   },
   "outputs": [],
   "source": [
    "def decimal_from_value(value):\n",
    "    \"\"\" helper function to convert a value to a Decimal datatype \"\"\"\n",
    "    return Decimal(value)\n",
    "\n",
    "\n",
    "def convert_data_types(col_headers, define_data_types):\n",
    "    \"\"\" helper method to lookup Dataset-JSON datatypes to Pandas datatypes \"\"\"\n",
    "    pandas_types = {}\n",
    "    for idx, header in enumerate(col_headers):\n",
    "        pandas_types[header] = lookup_pandas_data_type(define_data_types[idx])\n",
    "    return pandas_types\n",
    "\n",
    "\n",
    "def lookup_pandas_data_type(define_data_type):\n",
    "    \"\"\" helper method to convert a given Dataset-JSON datatype to a Panda datatype \"\"\"\n",
    "    if define_data_type == \"integer\":\n",
    "        return \"int64\"\n",
    "    elif define_data_type == \"string\":\n",
    "        return \"object\"\n",
    "    elif define_data_type == \"decimal\":\n",
    "        # store as object and convert on read to decimal\n",
    "        return \"object\"\n",
    "    elif define_data_type == \"float\" or define_data_type == \"double\":\n",
    "        return \"float64\"\n",
    "    elif define_data_type == \"datetime\":\n",
    "        return \"datetime64\"\n",
    "    else:\n",
    "        print(f\"unhandled Define-XML datatype: {define_data_type}. Assigning to object datatype.\")\n",
    "        return \"object\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stream the Dataset-JSON file\n",
    "Next we will stream, or iteratively parse, the Dataset-JSON file and write it out to CSV incrementally. Stream the Dataset-JSON file reading one record at a time (using ijson) and iteratively write a set of rows to the CSV file. Set missing VSREPNUM values to -99 to enable Pandas to set the datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:35:46.758461736Z",
     "start_time": "2023-08-18T17:35:46.706899749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total incremental rows printed: 1414\n"
     ]
    }
   ],
   "source": [
    "# read Dataset-JSON data rows iteratively and write them to the csv file\n",
    "DF_CHUNK_SIZE = 300\n",
    "CSV_FILE = 'data/incremental_vs.csv'\n",
    "def create_new_dataframe(rows):\n",
    "     idf = pd.DataFrame(rows, columns=variables)\n",
    "     idf.to_csv(CSV_FILE, index=True, encoding='utf-8', columns=variables)\n",
    "\n",
    "def append_to_dataframe(rows):\n",
    "     idf = pd.DataFrame(rows, columns=variables)\n",
    "     idf.to_csv(CSV_FILE, index=True, encoding='utf-8', header=None, mode='a')\n",
    "\n",
    "with open('data/vs.json', 'rb') as f:\n",
    "    row_set = []\n",
    "    rows = ijson.items(f, 'clinicalData.itemGroupData.IG.VS.itemData.item')\n",
    "    for row_count, row in enumerate(rows):\n",
    "        if row[16] is None:\n",
    "            row[16] = -99\n",
    "        row_set.append(row)\n",
    "        if len(row_set) == DF_CHUNK_SIZE:\n",
    "            idf = pd.DataFrame(row_set, columns=variables)\n",
    "            if row_count > DF_CHUNK_SIZE:\n",
    "                append_to_dataframe(row_set)\n",
    "            else:\n",
    "                create_new_dataframe(row_set)\n",
    "            row_set = []\n",
    "    if len(row_set) and row_count > DF_CHUNK_SIZE:\n",
    "        append_to_dataframe(row_set)\n",
    "    else:\n",
    "        create_new_dataframe(row_set)\n",
    "print(f\"total incremental rows printed: {row_count + 1}\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create dataframes from the CSV file and show memory usage\n",
    "Load the complete CSV file at once and convert the datatypes to pandas datatypes. Remove the datatype settings for the columns for which we have a converter. Print the memory usage for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:35:58.382649760Z",
     "start_time": "2023-08-18T17:35:58.338413533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 1.48 ms\n",
      "Type conversion took: 3.59 ms\n",
      "Parser memory cleanup took: 0.01 ms\n",
      "Rows in the incremental csv dataframe: 1414\n",
      "full csv memory usage: 260176 bytes\n"
     ]
    }
   ],
   "source": [
    "# load full csv with data types and date processing\n",
    "pandas_dtypes = convert_data_types(variables, data_types)\n",
    "# remove datatypes for which we will apply a converter\n",
    "pandas_dtypes.pop('VSSTRESN', None)\n",
    "pandas_dtypes.pop('VISITNUM', None)\n",
    "csv_df = pd.read_csv(CSV_FILE,\n",
    "                     index_col=0,\n",
    "                     dtype=pandas_dtypes,\n",
    "                     parse_dates=[\"VSDTC\"],\n",
    "                     converters={'VSSTRESN': decimal_from_value, 'VISITNUM': decimal_from_value},\n",
    "                     verbose=True,\n",
    "                     quoting=csv.QUOTE_NONE,\n",
    "                     on_bad_lines='warn',\n",
    "                     skip_blank_lines=False)\n",
    "print(f\"Rows in the incremental csv dataframe: {len(csv_df.axes[0])}\")\n",
    "print(f\"full csv memory usage: {csv_df.memory_usage().sum()} bytes\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To reduce the amount of memory used, this time load just 5 columns of the dataset. Show the top 5 records, the number of rows, and print the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:36:28.902665313Z",
     "start_time": "2023-08-18T17:36:28.856928909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 1.34 ms\n",
      "Type conversion took: 0.75 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "   ITEMGROUPDATASEQ   USUBJID                    VSTEST VSORRES VSORRESU\n",
      "0                 1  CDISC001  Diastolic Blood Pressure      71     mmHg\n",
      "1                 2  CDISC001  Diastolic Blood Pressure      71     mmHg\n",
      "2                 3  CDISC001  Diastolic Blood Pressure      83     mmHg\n",
      "3                 4  CDISC001  Diastolic Blood Pressure      79     mmHg\n",
      "4                 5  CDISC001  Diastolic Blood Pressure      68     mmHg\n",
      "\n",
      "5 column incremental dataframe row count: 1414\n",
      "5 columns csv memory usage: 56688 bytes\n"
     ]
    }
   ],
   "source": [
    "# load 5 columns only\n",
    "cols = ['ITEMGROUPDATASEQ', 'USUBJID', 'VSTEST', 'VSORRES', 'VSORRESU']\n",
    "col_dtypes = {col: pandas_dtypes[col] for col in cols}\n",
    "csv_df = pd.read_csv('data/incremental_vs.csv',\n",
    "                     usecols=cols,\n",
    "                     dtype=col_dtypes,\n",
    "                     on_bad_lines='warn',\n",
    "                     verbose=True,\n",
    "                     quoting=csv.QUOTE_NONE)\n",
    "print(csv_df.head(n=5), end='\\n\\n')\n",
    "print(f\"5 column incremental dataframe row count: {len(csv_df.axes[0])}\")\n",
    "print(f\"5 columns csv memory usage: {csv_df.memory_usage().sum()} bytes\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write the 5 column CSV to a compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:36:54.707052930Z",
     "start_time": "2023-08-18T17:36:54.661293404Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_df.to_csv('data/vs_zip.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the compressed 5 columns dataset and print the memory usage. Note that compressing the dataset saves disk storage, but not the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:36:59.570237434Z",
     "start_time": "2023-08-18T17:36:59.516475837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ITEMGROUPDATASEQ   USUBJID                    VSTEST VSORRES VSORRESU\n",
      "0                 1  CDISC001  Diastolic Blood Pressure      71     mmHg\n",
      "1                 2  CDISC001  Diastolic Blood Pressure      71     mmHg\n",
      "2                 3  CDISC001  Diastolic Blood Pressure      83     mmHg\n",
      "3                 4  CDISC001  Diastolic Blood Pressure      79     mmHg\n",
      "4                 5  CDISC001  Diastolic Blood Pressure      68     mmHg\n",
      "\n",
      "5 columns zipped csv memory usage: 67872 bytes\n"
     ]
    }
   ],
   "source": [
    "df_zip = pd.read_csv('data/vs_zip.csv.zip',\n",
    "                     index_col=0,\n",
    "                     dtype=pandas_dtypes,\n",
    "                     converters={'VSSTRESN': decimal_from_value, 'VISITNUM': decimal_from_value})\n",
    "print(df_zip.head(n=5), end='\\n\\n')\n",
    "print(f\"5 columns zipped csv memory usage: {df_zip.memory_usage().sum()} bytes\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert the CSV file to Dataset-JSON\n",
    "Next read in the 5 column VS dataset and write it out as Dataset-JSON. Start by creating a Dataset-JSON dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.460792875Z",
     "start_time": "2023-08-18T17:24:49.368953101Z"
    }
   },
   "outputs": [],
   "source": [
    "vs_5col_json = {\n",
    "  \"clinicalData\": {\n",
    "    \"studyOID\": \"hackathon.vs.reduced\",\n",
    "    \"metaDataVersionOID\": \"MDV.MSGv2.0.SDTMIG.3.3.VS.5COL\",\n",
    "    \"itemGroupData\": {\n",
    "      \"IG.VS\": {\n",
    "        \"records\": len(csv_df.axes[0]),\n",
    "        \"name\": \"VS\",\n",
    "        \"label\": \"Vital Signs\",\n",
    "        \"items\": [],\n",
    "        \"itemData\": []\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next read the 5 column VS dataset stored as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.461022071Z",
     "start_time": "2023-08-18T17:24:49.369154918Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_5col_df = pd.read_csv('data/incremental_vs.csv', usecols=cols, dtype=col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we set the column headers for the 5 columns in the dataset and the read in the rows from the dataframe and insert them into the itemData list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.462541941Z",
     "start_time": "2023-08-18T17:24:49.411675981Z"
    }
   },
   "outputs": [],
   "source": [
    "vs_5col_json[\"clinicalData\"][\"itemGroupData\"][\"IG.VS\"][\"items\"] = [var_defn for var_defn in dataset_attrs['items'] if var_defn['name'] in cols]\n",
    "vs_5col_json[\"clinicalData\"][\"itemGroupData\"][\"IG.VS\"][\"itemData\"] = csv_df.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the number of data rows being written to the Dataset-JSON file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 1414 dataset rows to Dataset-JSON\n"
     ]
    }
   ],
   "source": [
    "print(f\"writing {len(vs_5col_json['clinicalData']['itemGroupData']['IG.VS']['itemData'])} dataset rows to Dataset-JSON\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.463614792Z",
     "start_time": "2023-08-18T17:24:49.412009666Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, write the 5 column VS dataset to a Dataset-JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.463930318Z",
     "start_time": "2023-08-18T17:24:49.412215882Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/vs_5col.json\", \"w\") as fo:\n",
    "    json.dump(vs_5col_json, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-18T17:24:49.468734715Z",
     "start_time": "2023-08-18T17:24:49.412365352Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
